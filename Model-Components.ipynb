{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390ebc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa802d50",
   "metadata": {},
   "source": [
    "<b>1. Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8656b511",
   "metadata": {},
   "source": [
    "1.1 Tokenisation + Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17a24d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 995, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder = tiktoken.get_encoding('gpt2')\n",
    "Encoder.encode('Hello world!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61488eda",
   "metadata": {},
   "source": [
    "1.2 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b444170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Embedder, self).__init__()\n",
    "\n",
    "        self.token_embedder = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.position_embedder = nn.Embedding(config.sequence_length, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        batch_size, sequence_length = idx.size() if idx.dim == 2 else 1 , idx.size()[0]\n",
    "        pos = torch.arange(0, sequence_length, dtype=torch.long).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        token_embeddings = self.token_embedder(idx)\n",
    "        positional_embeddings = self.position_embedder(pos)\n",
    "\n",
    "        out = self.dropout(token_embeddings + positional_embeddings)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03bd8e26",
   "metadata": {},
   "source": [
    "<b>2. Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba4d6a3d",
   "metadata": {},
   "source": [
    "2.1 Multi-Head Self Attention (MHSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde07959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.num_heads = config.num_heads\n",
    "        self.qkv_bias = config.qkv_bias\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim, bias=self.qkv_bias)\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim, bias=self.qkv_bias)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim, bias=self.qkv_bias)\n",
    "\n",
    "        self.fc = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        sequence_length = x.shape[1]\n",
    "\n",
    "        #Reshape for q,k,v calculation\n",
    "        #q = x.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        #k = x.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        #v = x.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        #Calculation of q,k,v\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        q = q.view(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        v = v.view(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        #Reshape for Attention Calculation\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 2, 3, 1)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        #Calculate Scaled dot-product Attention\n",
    "        attention = torch.matmul(q, k)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = torch.matmul(attention, v)\n",
    "\n",
    "        #Reshape/Flatten for FFNN \n",
    "        out = attention.permute(0,2,1,3).contiguous().view(batch_size, sequence_length, -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e2f2ac",
   "metadata": {},
   "source": [
    "2.2 Feed Forward Neural Network (FFNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc96e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.expand = nn.Linear(config.embed_dim, 4 * config.embed_dim, bias=config.ffnn_bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.project = nn.Linear(4 * config.embed_dim, config.embed_dim, bias=config.ffnn_bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.expand(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.project(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9376fc6e",
   "metadata": {},
   "source": [
    "2.3 Layer Normalisation (LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a24a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LayerNorm, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(config.embed_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(config.embed_dim)) if config.layernorm_bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.layer_norm(x, self.weight.shape, self.weight, self.bias)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09d81b0e",
   "metadata": {},
   "source": [
    "Complete Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea881a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.MHSA = MultiHeadSelfAttention(config)\n",
    "        self.FFNN = FeedForwardNeuralNetwork(config)\n",
    "        self.LN1 = LayerNorm(config)\n",
    "        self.LN2 = LayerNorm(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.MHSA(x)\n",
    "        x = self.LN1(x)\n",
    "        x = x + self.FFNN(x)\n",
    "        x = self.LN2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05a248fb",
   "metadata": {},
   "source": [
    "<b>3. Post-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcc7151f",
   "metadata": {},
   "source": [
    "3.1 Transformer Output to Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa6979d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostTransformerLayers(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PostTransformerLayers, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddb83f25",
   "metadata": {},
   "source": [
    "3.2 Next Token Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3cef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PostProcess(probs, topK, temperature):\n",
    "    sorted_probs, sorted_indices = torch.topk(probs, topK)\n",
    "    flattened_probs, flattened_indices = sorted_probs.view(-1), sorted_indices.view(-1)\n",
    "    next_token = torch.multinomial(torch.softmax(flattened_probs / temperature, dim=0), 1)\n",
    "    next_token = flattened_indices[next_token]\n",
    "    return next_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26788d0",
   "metadata": {},
   "source": [
    "3.3 Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4d49b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder.decode([15496, 995, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f816218",
   "metadata": {},
   "source": [
    "<b>4. Config Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c8d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class pashkoConfig:\n",
    "    sequence_length: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    embed_dim: int = 768\n",
    "\n",
    "    encoder = 'gpt2'\n",
    "\n",
    "    batch_size: int = 64\n",
    "\n",
    "    num_heads: int = 12\n",
    "    num_blocks: int = 12\n",
    "\n",
    "    dropout: float = 0.0\n",
    "\n",
    "    ffnn_bias: bool = False\n",
    "    qkv_bias: bool = False\n",
    "    layernorm_bias = False\n",
    "\n",
    "    topK: int = 10\n",
    "    temperature: float = 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3da29502",
   "metadata": {},
   "source": [
    "<b> 5. Complete GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aadc1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pashko(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(pashko, self).__init__()\n",
    "\n",
    "        self.Encoder = tiktoken.get_encoding('gpt2')\n",
    "        self.Embedder = Embedder(config)\n",
    "        \n",
    "        self.Blocks = [Block(config) for _ in range(config.num_blocks)]\n",
    "        for i, block in enumerate(self.Blocks):\n",
    "            self.add_module(f'Transformer Block {i}', block)\n",
    "\n",
    "        self.PostTransformer = PostTransformerLayers(config)\n",
    "\n",
    "        self.LossFunction = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        #Weight Initialisation according to GPT2 Paper\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "        #Weight tying Embedding to Final Linear\n",
    "        self.Embedder.token_embedder.weight = self.PostTransformer.fc.weight\n",
    "\n",
    "    #Different types of generation.\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.Embedder(x) #Embeddings\n",
    "\n",
    "        for Transformer in self.Blocks: #Transformer Block\n",
    "            x = Transformer(x)\n",
    "        \n",
    "        x = self.PostTransformer(x) #Post-Transformer Layers\n",
    "\n",
    "        x = x.view(-1, self.config.vocab_size) #Loss Calculation\n",
    "        targets = targets.view(-1)\n",
    "        loss = self.LossFunction(x, targets)\n",
    "        return x, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, x):\n",
    "        x = self.Embedder(x) #Embeddings\n",
    "\n",
    "        for Transformer in self.Blocks: #Transformer Block\n",
    "            x = Transformer(x)\n",
    "        \n",
    "        x = self.PostTransformer(x) #Post-Transformer Layers\n",
    "        token = PostProcess(x, self.config.topK, self.config.temperature)\n",
    "        return token\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, context, max_new_tokens=64, show=True):\n",
    "        response = []\n",
    "\n",
    "        x = torch.LongTensor(Encoder.encode(context))\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            x = x if x.size(0) <= self.config.sequence_length else x[:, -self.config.sequence_length:]\n",
    "\n",
    "            next_token = self.inference(x)\n",
    "\n",
    "            response.append(next_token.numpy()[0])\n",
    "\n",
    "            if show:\n",
    "                print(self.Encoder.decode(next_token.numpy()), end='', flush=True)\n",
    "\n",
    "            x = torch.cat((x, next_token), dim=0)\n",
    "    #Utils\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def num_params(self, embedding=False):\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        if not embedding:\n",
    "            num_params -= self.Embedder.position_embedder.weight.numel()\n",
    "        return \"{:.2f}M\".format(num_params / 1000000), num_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5f0ce2c",
   "metadata": {},
   "source": [
    "<b>6. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f9c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pashkoConfig()\n",
    "Pashko = pashko(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efc43842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('123.60M', 123595776)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pashko.num_params() #Correct Number of Parameters due to weight tying."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f4d355c",
   "metadata": {},
   "source": [
    "Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58a54015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─Embedder: 1-1                               --\n",
      "|    └─Embedding: 2-1                         38,633,472\n",
      "|    └─Embedding: 2-2                         786,432\n",
      "|    └─Dropout: 2-3                           --\n",
      "├─Block: 1-2                                  --\n",
      "|    └─MultiHeadSelfAttention: 2-4            --\n",
      "|    |    └─Linear: 3-1                       589,824\n",
      "|    |    └─Linear: 3-2                       589,824\n",
      "|    |    └─Linear: 3-3                       589,824\n",
      "|    |    └─Linear: 3-4                       590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-5          --\n",
      "|    |    └─Linear: 3-5                       2,359,296\n",
      "|    |    └─GELU: 3-6                         --\n",
      "|    |    └─Linear: 3-7                       2,359,296\n",
      "|    |    └─Dropout: 3-8                      --\n",
      "|    └─LayerNorm: 2-6                         768\n",
      "|    └─LayerNorm: 2-7                         768\n",
      "├─Block: 1-3                                  --\n",
      "|    └─MultiHeadSelfAttention: 2-8            --\n",
      "|    |    └─Linear: 3-9                       589,824\n",
      "|    |    └─Linear: 3-10                      589,824\n",
      "|    |    └─Linear: 3-11                      589,824\n",
      "|    |    └─Linear: 3-12                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-9          --\n",
      "|    |    └─Linear: 3-13                      2,359,296\n",
      "|    |    └─GELU: 3-14                        --\n",
      "|    |    └─Linear: 3-15                      2,359,296\n",
      "|    |    └─Dropout: 3-16                     --\n",
      "|    └─LayerNorm: 2-10                        768\n",
      "|    └─LayerNorm: 2-11                        768\n",
      "├─Block: 1-4                                  --\n",
      "|    └─MultiHeadSelfAttention: 2-12           --\n",
      "|    |    └─Linear: 3-17                      589,824\n",
      "|    |    └─Linear: 3-18                      589,824\n",
      "|    |    └─Linear: 3-19                      589,824\n",
      "|    |    └─Linear: 3-20                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-13         --\n",
      "|    |    └─Linear: 3-21                      2,359,296\n",
      "|    |    └─GELU: 3-22                        --\n",
      "|    |    └─Linear: 3-23                      2,359,296\n",
      "|    |    └─Dropout: 3-24                     --\n",
      "|    └─LayerNorm: 2-14                        768\n",
      "|    └─LayerNorm: 2-15                        768\n",
      "├─Block: 1-5                                  --\n",
      "|    └─MultiHeadSelfAttention: 2-16           --\n",
      "|    |    └─Linear: 3-25                      589,824\n",
      "|    |    └─Linear: 3-26                      589,824\n",
      "|    |    └─Linear: 3-27                      589,824\n",
      "|    |    └─Linear: 3-28                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-17         --\n",
      "|    |    └─Linear: 3-29                      2,359,296\n",
      "|    |    └─GELU: 3-30                        --\n",
      "|    |    └─Linear: 3-31                      2,359,296\n",
      "|    |    └─Dropout: 3-32                     --\n",
      "|    └─LayerNorm: 2-18                        768\n",
      "|    └─LayerNorm: 2-19                        768\n",
      "├─Block: 1-6                                  --\n",
      "|    └─MultiHeadSelfAttention: 2-20           --\n",
      "|    |    └─Linear: 3-33                      589,824\n",
      "|    |    └─Linear: 3-34                      589,824\n",
      "|    |    └─Linear: 3-35                      589,824\n",
      "|    |    └─Linear: 3-36                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-21         --\n",
      "|    |    └─Linear: 3-37                      2,359,296\n",
      "|    |    └─GELU: 3-38                        --\n",
      "|    |    └─Linear: 3-39                      2,359,296\n",
      "|    |    └─Dropout: 3-40                     --\n",
      "|    └─LayerNorm: 2-22                        768\n",
      "|    └─LayerNorm: 2-23                        768\n",
      "├─Block: 1-7                                  --\n",
      "|    └─MultiHeadSelfAttention: 2-24           --\n",
      "|    |    └─Linear: 3-41                      589,824\n",
      "|    |    └─Linear: 3-42                      589,824\n",
      "|    |    └─Linear: 3-43                      589,824\n",
      "|    |    └─Linear: 3-44                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-25         --\n",
      "|    |    └─Linear: 3-45                      2,359,296\n",
      "|    |    └─GELU: 3-46                        --\n",
      "|    |    └─Linear: 3-47                      2,359,296\n",
      "|    |    └─Dropout: 3-48                     --\n",
      "|    └─LayerNorm: 2-26                        768\n",
      "|    └─LayerNorm: 2-27                        768\n",
      "├─Block: 1-8                                  --\n",
      "|    └─MultiHeadSelfAttention: 2-28           --\n",
      "|    |    └─Linear: 3-49                      589,824\n",
      "|    |    └─Linear: 3-50                      589,824\n",
      "|    |    └─Linear: 3-51                      589,824\n",
      "|    |    └─Linear: 3-52                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-29         --\n",
      "|    |    └─Linear: 3-53                      2,359,296\n",
      "|    |    └─GELU: 3-54                        --\n",
      "|    |    └─Linear: 3-55                      2,359,296\n",
      "|    |    └─Dropout: 3-56                     --\n",
      "|    └─LayerNorm: 2-30                        768\n",
      "|    └─LayerNorm: 2-31                        768\n",
      "├─Block: 1-9                                  --\n",
      "|    └─MultiHeadSelfAttention: 2-32           --\n",
      "|    |    └─Linear: 3-57                      589,824\n",
      "|    |    └─Linear: 3-58                      589,824\n",
      "|    |    └─Linear: 3-59                      589,824\n",
      "|    |    └─Linear: 3-60                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-33         --\n",
      "|    |    └─Linear: 3-61                      2,359,296\n",
      "|    |    └─GELU: 3-62                        --\n",
      "|    |    └─Linear: 3-63                      2,359,296\n",
      "|    |    └─Dropout: 3-64                     --\n",
      "|    └─LayerNorm: 2-34                        768\n",
      "|    └─LayerNorm: 2-35                        768\n",
      "├─Block: 1-10                                 --\n",
      "|    └─MultiHeadSelfAttention: 2-36           --\n",
      "|    |    └─Linear: 3-65                      589,824\n",
      "|    |    └─Linear: 3-66                      589,824\n",
      "|    |    └─Linear: 3-67                      589,824\n",
      "|    |    └─Linear: 3-68                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-37         --\n",
      "|    |    └─Linear: 3-69                      2,359,296\n",
      "|    |    └─GELU: 3-70                        --\n",
      "|    |    └─Linear: 3-71                      2,359,296\n",
      "|    |    └─Dropout: 3-72                     --\n",
      "|    └─LayerNorm: 2-38                        768\n",
      "|    └─LayerNorm: 2-39                        768\n",
      "├─Block: 1-11                                 --\n",
      "|    └─MultiHeadSelfAttention: 2-40           --\n",
      "|    |    └─Linear: 3-73                      589,824\n",
      "|    |    └─Linear: 3-74                      589,824\n",
      "|    |    └─Linear: 3-75                      589,824\n",
      "|    |    └─Linear: 3-76                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-41         --\n",
      "|    |    └─Linear: 3-77                      2,359,296\n",
      "|    |    └─GELU: 3-78                        --\n",
      "|    |    └─Linear: 3-79                      2,359,296\n",
      "|    |    └─Dropout: 3-80                     --\n",
      "|    └─LayerNorm: 2-42                        768\n",
      "|    └─LayerNorm: 2-43                        768\n",
      "├─Block: 1-12                                 --\n",
      "|    └─MultiHeadSelfAttention: 2-44           --\n",
      "|    |    └─Linear: 3-81                      589,824\n",
      "|    |    └─Linear: 3-82                      589,824\n",
      "|    |    └─Linear: 3-83                      589,824\n",
      "|    |    └─Linear: 3-84                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-45         --\n",
      "|    |    └─Linear: 3-85                      2,359,296\n",
      "|    |    └─GELU: 3-86                        --\n",
      "|    |    └─Linear: 3-87                      2,359,296\n",
      "|    |    └─Dropout: 3-88                     --\n",
      "|    └─LayerNorm: 2-46                        768\n",
      "|    └─LayerNorm: 2-47                        768\n",
      "├─Block: 1-13                                 --\n",
      "|    └─MultiHeadSelfAttention: 2-48           --\n",
      "|    |    └─Linear: 3-89                      589,824\n",
      "|    |    └─Linear: 3-90                      589,824\n",
      "|    |    └─Linear: 3-91                      589,824\n",
      "|    |    └─Linear: 3-92                      590,592\n",
      "|    └─FeedForwardNeuralNetwork: 2-49         --\n",
      "|    |    └─Linear: 3-93                      2,359,296\n",
      "|    |    └─GELU: 3-94                        --\n",
      "|    |    └─Linear: 3-95                      2,359,296\n",
      "|    |    └─Dropout: 3-96                     --\n",
      "|    └─LayerNorm: 2-50                        768\n",
      "|    └─LayerNorm: 2-51                        768\n",
      "├─PostTransformerLayers: 1-14                 --\n",
      "|    └─Linear: 2-52                           38,633,472\n",
      "├─CrossEntropyLoss: 1-15                      --\n",
      "======================================================================\n",
      "Total params: 163,015,680\n",
      "Trainable params: 163,015,680\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─Embedder: 1-1                               --\n",
       "|    └─Embedding: 2-1                         38,633,472\n",
       "|    └─Embedding: 2-2                         786,432\n",
       "|    └─Dropout: 2-3                           --\n",
       "├─Block: 1-2                                  --\n",
       "|    └─MultiHeadSelfAttention: 2-4            --\n",
       "|    |    └─Linear: 3-1                       589,824\n",
       "|    |    └─Linear: 3-2                       589,824\n",
       "|    |    └─Linear: 3-3                       589,824\n",
       "|    |    └─Linear: 3-4                       590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-5          --\n",
       "|    |    └─Linear: 3-5                       2,359,296\n",
       "|    |    └─GELU: 3-6                         --\n",
       "|    |    └─Linear: 3-7                       2,359,296\n",
       "|    |    └─Dropout: 3-8                      --\n",
       "|    └─LayerNorm: 2-6                         768\n",
       "|    └─LayerNorm: 2-7                         768\n",
       "├─Block: 1-3                                  --\n",
       "|    └─MultiHeadSelfAttention: 2-8            --\n",
       "|    |    └─Linear: 3-9                       589,824\n",
       "|    |    └─Linear: 3-10                      589,824\n",
       "|    |    └─Linear: 3-11                      589,824\n",
       "|    |    └─Linear: 3-12                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-9          --\n",
       "|    |    └─Linear: 3-13                      2,359,296\n",
       "|    |    └─GELU: 3-14                        --\n",
       "|    |    └─Linear: 3-15                      2,359,296\n",
       "|    |    └─Dropout: 3-16                     --\n",
       "|    └─LayerNorm: 2-10                        768\n",
       "|    └─LayerNorm: 2-11                        768\n",
       "├─Block: 1-4                                  --\n",
       "|    └─MultiHeadSelfAttention: 2-12           --\n",
       "|    |    └─Linear: 3-17                      589,824\n",
       "|    |    └─Linear: 3-18                      589,824\n",
       "|    |    └─Linear: 3-19                      589,824\n",
       "|    |    └─Linear: 3-20                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-13         --\n",
       "|    |    └─Linear: 3-21                      2,359,296\n",
       "|    |    └─GELU: 3-22                        --\n",
       "|    |    └─Linear: 3-23                      2,359,296\n",
       "|    |    └─Dropout: 3-24                     --\n",
       "|    └─LayerNorm: 2-14                        768\n",
       "|    └─LayerNorm: 2-15                        768\n",
       "├─Block: 1-5                                  --\n",
       "|    └─MultiHeadSelfAttention: 2-16           --\n",
       "|    |    └─Linear: 3-25                      589,824\n",
       "|    |    └─Linear: 3-26                      589,824\n",
       "|    |    └─Linear: 3-27                      589,824\n",
       "|    |    └─Linear: 3-28                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-17         --\n",
       "|    |    └─Linear: 3-29                      2,359,296\n",
       "|    |    └─GELU: 3-30                        --\n",
       "|    |    └─Linear: 3-31                      2,359,296\n",
       "|    |    └─Dropout: 3-32                     --\n",
       "|    └─LayerNorm: 2-18                        768\n",
       "|    └─LayerNorm: 2-19                        768\n",
       "├─Block: 1-6                                  --\n",
       "|    └─MultiHeadSelfAttention: 2-20           --\n",
       "|    |    └─Linear: 3-33                      589,824\n",
       "|    |    └─Linear: 3-34                      589,824\n",
       "|    |    └─Linear: 3-35                      589,824\n",
       "|    |    └─Linear: 3-36                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-21         --\n",
       "|    |    └─Linear: 3-37                      2,359,296\n",
       "|    |    └─GELU: 3-38                        --\n",
       "|    |    └─Linear: 3-39                      2,359,296\n",
       "|    |    └─Dropout: 3-40                     --\n",
       "|    └─LayerNorm: 2-22                        768\n",
       "|    └─LayerNorm: 2-23                        768\n",
       "├─Block: 1-7                                  --\n",
       "|    └─MultiHeadSelfAttention: 2-24           --\n",
       "|    |    └─Linear: 3-41                      589,824\n",
       "|    |    └─Linear: 3-42                      589,824\n",
       "|    |    └─Linear: 3-43                      589,824\n",
       "|    |    └─Linear: 3-44                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-25         --\n",
       "|    |    └─Linear: 3-45                      2,359,296\n",
       "|    |    └─GELU: 3-46                        --\n",
       "|    |    └─Linear: 3-47                      2,359,296\n",
       "|    |    └─Dropout: 3-48                     --\n",
       "|    └─LayerNorm: 2-26                        768\n",
       "|    └─LayerNorm: 2-27                        768\n",
       "├─Block: 1-8                                  --\n",
       "|    └─MultiHeadSelfAttention: 2-28           --\n",
       "|    |    └─Linear: 3-49                      589,824\n",
       "|    |    └─Linear: 3-50                      589,824\n",
       "|    |    └─Linear: 3-51                      589,824\n",
       "|    |    └─Linear: 3-52                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-29         --\n",
       "|    |    └─Linear: 3-53                      2,359,296\n",
       "|    |    └─GELU: 3-54                        --\n",
       "|    |    └─Linear: 3-55                      2,359,296\n",
       "|    |    └─Dropout: 3-56                     --\n",
       "|    └─LayerNorm: 2-30                        768\n",
       "|    └─LayerNorm: 2-31                        768\n",
       "├─Block: 1-9                                  --\n",
       "|    └─MultiHeadSelfAttention: 2-32           --\n",
       "|    |    └─Linear: 3-57                      589,824\n",
       "|    |    └─Linear: 3-58                      589,824\n",
       "|    |    └─Linear: 3-59                      589,824\n",
       "|    |    └─Linear: 3-60                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-33         --\n",
       "|    |    └─Linear: 3-61                      2,359,296\n",
       "|    |    └─GELU: 3-62                        --\n",
       "|    |    └─Linear: 3-63                      2,359,296\n",
       "|    |    └─Dropout: 3-64                     --\n",
       "|    └─LayerNorm: 2-34                        768\n",
       "|    └─LayerNorm: 2-35                        768\n",
       "├─Block: 1-10                                 --\n",
       "|    └─MultiHeadSelfAttention: 2-36           --\n",
       "|    |    └─Linear: 3-65                      589,824\n",
       "|    |    └─Linear: 3-66                      589,824\n",
       "|    |    └─Linear: 3-67                      589,824\n",
       "|    |    └─Linear: 3-68                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-37         --\n",
       "|    |    └─Linear: 3-69                      2,359,296\n",
       "|    |    └─GELU: 3-70                        --\n",
       "|    |    └─Linear: 3-71                      2,359,296\n",
       "|    |    └─Dropout: 3-72                     --\n",
       "|    └─LayerNorm: 2-38                        768\n",
       "|    └─LayerNorm: 2-39                        768\n",
       "├─Block: 1-11                                 --\n",
       "|    └─MultiHeadSelfAttention: 2-40           --\n",
       "|    |    └─Linear: 3-73                      589,824\n",
       "|    |    └─Linear: 3-74                      589,824\n",
       "|    |    └─Linear: 3-75                      589,824\n",
       "|    |    └─Linear: 3-76                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-41         --\n",
       "|    |    └─Linear: 3-77                      2,359,296\n",
       "|    |    └─GELU: 3-78                        --\n",
       "|    |    └─Linear: 3-79                      2,359,296\n",
       "|    |    └─Dropout: 3-80                     --\n",
       "|    └─LayerNorm: 2-42                        768\n",
       "|    └─LayerNorm: 2-43                        768\n",
       "├─Block: 1-12                                 --\n",
       "|    └─MultiHeadSelfAttention: 2-44           --\n",
       "|    |    └─Linear: 3-81                      589,824\n",
       "|    |    └─Linear: 3-82                      589,824\n",
       "|    |    └─Linear: 3-83                      589,824\n",
       "|    |    └─Linear: 3-84                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-45         --\n",
       "|    |    └─Linear: 3-85                      2,359,296\n",
       "|    |    └─GELU: 3-86                        --\n",
       "|    |    └─Linear: 3-87                      2,359,296\n",
       "|    |    └─Dropout: 3-88                     --\n",
       "|    └─LayerNorm: 2-46                        768\n",
       "|    └─LayerNorm: 2-47                        768\n",
       "├─Block: 1-13                                 --\n",
       "|    └─MultiHeadSelfAttention: 2-48           --\n",
       "|    |    └─Linear: 3-89                      589,824\n",
       "|    |    └─Linear: 3-90                      589,824\n",
       "|    |    └─Linear: 3-91                      589,824\n",
       "|    |    └─Linear: 3-92                      590,592\n",
       "|    └─FeedForwardNeuralNetwork: 2-49         --\n",
       "|    |    └─Linear: 3-93                      2,359,296\n",
       "|    |    └─GELU: 3-94                        --\n",
       "|    |    └─Linear: 3-95                      2,359,296\n",
       "|    |    └─Dropout: 3-96                     --\n",
       "|    └─LayerNorm: 2-50                        768\n",
       "|    └─LayerNorm: 2-51                        768\n",
       "├─PostTransformerLayers: 1-14                 --\n",
       "|    └─Linear: 2-52                           38,633,472\n",
       "├─CrossEntropyLoss: 1-15                      --\n",
       "======================================================================\n",
       "Total params: 163,015,680\n",
       "Trainable params: 163,015,680\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Pashko, input_size=(config.batch_size,config.sequence_length))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
